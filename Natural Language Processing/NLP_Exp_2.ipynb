{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDE5r-5bxQcB",
        "outputId": "d69cb0a1-2063-4763-d916-ca8d4094366a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "   ---------------------------------------- 0.0/345.1 kB ? eta -:--:--\n",
            "   --------------------------- ------------ 235.5/345.1 kB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 345.1/345.1 kB 5.4 MB/s eta 0:00:00\n",
            "Downloading pyahocorasick-2.2.0-cp312-cp312-win_amd64.whl (35 kB)\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "%pip install contractions\n",
        "import contractions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paYpkWETySJ8",
        "outputId": "2aa94524-d6bc-4732-b83e-49895cf627ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\dhwaj\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\dhwaj\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\dhwaj\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\dhwaj\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\dhwaj\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\dhwaj\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\dhwaj\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haSUmbvH0SdS"
      },
      "source": [
        "### Removing HTML tags from the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjS7r0r0yY01",
        "outputId": "391b7295-7b93-4bfd-b9f3-7b1c8fc73bbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TitleSome content here.\n"
          ]
        }
      ],
      "source": [
        "def remove_html_tags_bs4(html_doc):\n",
        "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "html_doc = \"<html><body><h1>Title</h1><p>Some content here.</p></body></html>\"\n",
        "clean_text = remove_html_tags_bs4(html_doc)\n",
        "print(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rBd0QBciztOc"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "<p>Here's a revised demonstration featuring <b>HTML tags</b> as well as numerals like 987.</p>\n",
        "This sample covers contractions such as won't and she's, also accented letters: naïve and coöperate.\n",
        "Notice multiple     spaces in various places; how does the pipeline deal with extra punctuation??\n",
        "Words like eating, drove, and best display verb and adjective variations too.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IVqcUh60Xui",
        "outputId": "80aa0ad8-9239-4036-a6e0-8d1d9d9c6e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' ', '', 'p', '', 'h', 'e', 'r', 'e', '', 's', ' ', 'a', ' ', 'r', 'e', 'v', 'i', 's', 'e', 'd', ' ', 'd', 'e', 'm', 'o', 'n', 's', 't', 'r', 'a', 't', 'i', 'o', 'n', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'i', 'n', 'g', ' ', '', 'b', '', 'h', 't', 'm', 'l', ' ', 't', 'a', 'g', 's', '', '', 'b', '', ' ', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'a', 's', ' ', 'n', 'u', 'm', 'e', 'r', 'a', 'l', 's', ' ', 'l', 'i', 'k', 'e', ' ', '', '', '', '', '', '', 'p', '', ' ', 't', 'h', 'i', 's', ' ', 's', 'a', 'm', 'p', 'l', 'e', ' ', 'c', 'o', 'v', 'e', 'r', 's', ' ', 'c', 'o', 'n', 't', 'r', 'a', 'c', 't', 'i', 'o', 'n', 's', ' ', 's', 'u', 'c', 'h', ' ', 'a', 's', ' ', 'w', 'o', 'n', '', 't', ' ', 'a', 'n', 'd', ' ', 's', 'h', 'e', '', 's', '', ' ', 'a', 'l', 's', 'o', ' ', 'a', 'c', 'c', 'e', 'n', 't', 'e', 'd', ' ', 'l', 'e', 't', 't', 'e', 'r', 's', '', ' ', 'n', 'a', 'ï', 'v', 'e', ' ', 'a', 'n', 'd', ' ', 'c', 'o', 'ö', 'p', 'e', 'r', 'a', 't', 'e', '', ' ', 'n', 'o', 't', 'i', 'c', 'e', ' ', 'm', 'u', 'l', 't', 'i', 'p', 'l', 'e', ' ', ' ', ' ', ' ', ' ', 's', 'p', 'a', 'c', 'e', 's', ' ', 'i', 'n', ' ', 'v', 'a', 'r', 'i', 'o', 'u', 's', ' ', 'p', 'l', 'a', 'c', 'e', 's', '', ' ', 'h', 'o', 'w', ' ', 'd', 'o', 'e', 's', ' ', 't', 'h', 'e', ' ', 'p', 'i', 'p', 'e', 'l', 'i', 'n', 'e', ' ', 'd', 'e', 'a', 'l', ' ', 'w', 'i', 't', 'h', ' ', 'e', 'x', 't', 'r', 'a', ' ', 'p', 'u', 'n', 'c', 't', 'u', 'a', 't', 'i', 'o', 'n', '', '', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'l', 'i', 'k', 'e', ' ', 'e', 'a', 't', 'i', 'n', 'g', '', ' ', 'd', 'r', 'o', 'v', 'e', '', ' ', 'a', 'n', 'd', ' ', 'b', 'e', 's', 't', ' ', 'd', 'i', 's', 'p', 'l', 'a', 'y', ' ', 'v', 'e', 'r', 'b', ' ', 'a', 'n', 'd', ' ', 'a', 'd', 'j', 'e', 'c', 't', 'i', 'v', 'e', ' ', 'v', 'a', 'r', 'i', 'a', 't', 'i', 'o', 'n', 's', ' ', 't', 'o', 'o', '', ' ']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
        "    return text\n",
        "\n",
        "cleaned_text = [clean_text(doc) for doc in text]\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3AhwBFi09Ax"
      },
      "source": [
        "### Remove extra whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocTZ_k9l0fJ9",
        "outputId": "dad4f4e2-aa45-40c3-8736-50766b0e8ce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The strings after extra space removal : GfG is good website\n",
            "The strings after extra space removal : GfG is good website\n"
          ]
        }
      ],
      "source": [
        "test_str = \"GfG is      good         website\"\n",
        "res = re.sub(' +', ' ', test_str) #one way\n",
        "res2 = \" \".join(test_str.split()) #another way\n",
        "print(\"The strings after extra space removal : \" + str(res))\n",
        "print(\"The strings after extra space removal : \" + str(res2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9674de43",
        "outputId": "227c4f0a-5ef2-4dc4-89db-be8eb8d361f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The string after extra space removal : GfG is good website\n"
          ]
        }
      ],
      "source": [
        "def removing_white_spaces(text):\n",
        "    # Using regex to replace multiple spaces with a single space\n",
        "    # Alternatively, using split and join\n",
        "    res = \" \".join(text.split())\n",
        "    return res\n",
        "\n",
        "# Example usage:\n",
        "test_str = \"GfG is      good         website\"\n",
        "cleaned_str = removing_white_spaces(test_str)\n",
        "print(\"The string after extra space removal : \" + cleaned_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t4CdYKI2X7N"
      },
      "source": [
        "### Expand contractions\n",
        "\n",
        "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfW8DTq51-vI",
        "outputId": "c9eff081-333c-427a-8ae0-451bab799881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: I'll be there within 5 min. Shouldn't you be there too?\n",
            "          I'd love to see u there my dear. It's awesome to meet new friends.\n",
            "          We've been waiting for this day for so long.\n",
            "Expanded_text: I will be there within 5 min. Should not you be there too? I would love to see you there my dear. It is awesome to meet new friends. We have been waiting for this day for so long.\n"
          ]
        }
      ],
      "source": [
        "# import library\n",
        "import contractions\n",
        "# contracted text\n",
        "text = '''I'll be there within 5 min. Shouldn't you be there too?\n",
        "          I'd love to see u there my dear. It's awesome to meet new friends.\n",
        "          We've been waiting for this day for so long.'''\n",
        "\n",
        "# creating an empty list\n",
        "expanded_words = []\n",
        "for word in text.split():\n",
        "  # using contractions.fix to expand the shortened words\n",
        "  expanded_words.append(contractions.fix(word))\n",
        "\n",
        "expanded_text = ' '.join(expanded_words)\n",
        "print('Original text: ' + text)\n",
        "print('Expanded_text: ' + expanded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqUl_s-24kBe"
      },
      "source": [
        "### Convert accented characters to ASCII characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Dz4Zid7627Sp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cafe is located in Saint-Raphael.\n"
          ]
        }
      ],
      "source": [
        "text = \"The café is located in Saint-Raphaël.\"\n",
        "# Normalize to NFD (Normalization Form D) to decompose characters\n",
        "normalized_text = unicodedata.normalize('NFD', text)\n",
        "# Encode to ASCII and ignore non-ASCII characters (which will be the diacritics)\n",
        "ascii_text = normalized_text.encode('ascii', 'ignore').decode('utf-8')\n",
        "print(ascii_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db1d557d",
        "outputId": "eaede977-9e42-44e1-ab75-37c5d1df54e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cafe is located in Saint-Raphael.\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    # Normalize to NFD (Normalization Form D) to decompose characters\n",
        "    normalized_text = unicodedata.normalize('NFD', text)\n",
        "    # Encode to ASCII and ignore non-ASCII characters (which will be the diacritics)\n",
        "    ascii_text = normalized_text.encode('ascii', 'ignore').decode('utf-8')\n",
        "    return ascii_text\n",
        "\n",
        "# Example usage:\n",
        "text = \"The café is located in Saint-Raphaël.\"\n",
        "cleaned_text = remove_accented_chars(text)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76RrmREx4E4N",
        "outputId": "48ecc1a8-dd15-4af8-9471-f46e74567ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Le cafe est situe a Saint-Raphael. C'est un endroit tres agreable pour prendre un verre ou manger un morceau. J'adore l'atmosphere decontractee et les delicieuses patisseries qu'ils proposent.\n"
          ]
        }
      ],
      "source": [
        "text2 = \"Le café est situé à Saint-Raphaël. C'est un endroit très agréable pour prendre un verre ou manger un morceau. J'adore l'atmosphère décontractée et les délicieuses pâtisseries qu'ils proposent.\"\n",
        "print(remove_accented_chars(text2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDlDBba04n2N"
      },
      "source": [
        "### Remove special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCXjp6ZR4frw",
        "outputId": "78d2342b-09fe-46dc-c55b-38ff39366f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataScienceRocks123\n"
          ]
        }
      ],
      "source": [
        "s = \"Data!@Science#Rocks123\"\n",
        "res = re.sub(r'[^a-zA-Z0-9]', '', s)#re.sub(pattern, replace, string)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv5nIhVV5jTA"
      },
      "source": [
        "### Lowercase all texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "womHoByo4zbi",
        "outputId": "bb7f656e-b3a8-47c5-96f4-ad41b6f08de9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"hey, did you know that the summer break is coming? amazing right !! it's only 5 more days !!\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def text_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\";\n",
        "text_lowercase(input_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuAsBgEV5s4P"
      },
      "source": [
        "### Remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p34Fr6-i5qhL",
        "outputId": "61e83ca8-c372-4d09-effa-74250b34d175"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'There are  balls in these  bags, and  in the other  bags.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def remove_numbers(text):\n",
        "    result = re.sub(r'\\d+', '', text)\n",
        "    return result\n",
        "\n",
        "input_str = \"There are 3 balls in these 102020 bags, and 12 in the other 10000 bags.\"\n",
        "remove_numbers(input_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXOu0R3X5-ap"
      },
      "source": [
        "### Remove stopwords\n",
        "Natural language processing tasks often involve filtering out commonly occurring words that provide no or very little semantic value to text analysis. These words are known as stopwords include articles, prepositions and pronouns like \"the\", \"and\", \"is\" and \"in.\" While they seem insignificant, proper stopword handling can dramatically impact the performance and accuracy of NLP applications.\n",
        "\n",
        "Stopwords: \"the\" and \"over\"\n",
        "\n",
        "Content words: \"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog\"\n",
        "\n",
        "Tasks that benefit from stopword removal:\n",
        "Text classification and sentiment analysis\n",
        "Information retrieval and search engines\n",
        "Topic modelling and clustering\n",
        "Keyword extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56c0af9f",
        "outputId": "9ac8ebd4-6ae3-4a3c-e047-1091b4c05f39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: ['this', 'is', 'a', 'sample', 'sentence', 'showing', 'stopword', 'removal', '.']\n",
            "Filtered: ['sample', 'sentence', 'showing', 'stopword', 'removal', '.']\n"
          ]
        }
      ],
      "source": [
        "def remove_stopwords(text):\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Tokenize and lowercase the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "# Example usage:\n",
        "text = \"This is a sample sentence showing stopword removal.\"\n",
        "filtered_text = remove_stopwords(text)\n",
        "print(\"Original:\", word_tokenize(text.lower()))\n",
        "print(\"Filtered:\", filtered_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRxUDDg78XQJ",
        "outputId": "c48184b0-6a44-4425-fe7c-056eb72f11e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: ['this', 'is', 'a', 'much', 'longer', 'sentence', 'that', 'is', 'being', 'used', 'as', 'a', 'sample', 'to', 'show', 'how', 'stopword', 'removal', 'works', 'on', 'a', 'more', 'complex', 'piece', 'of', 'text', '.']\n",
            "Filtered: ['much', 'longer', 'sentence', 'used', 'sample', 'show', 'stopword', 'removal', 'works', 'complex', 'piece', 'text', '.']\n"
          ]
        }
      ],
      "source": [
        "text = \"This is a much longer sentence that is being used as a sample to show how stopword removal works on a more complex piece of text.\"\n",
        "filtered_text = remove_stopwords(text)\n",
        "print(\"Original:\", word_tokenize(text.lower()))\n",
        "print(\"Filtered:\", filtered_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTPdlH4O8qEL"
      },
      "source": [
        "### Lemmatization\n",
        "Lemmatization is an important text pre-processing technique in Natural Language Processing (NLP) that reduces words to their base form known as a \"lemma.\" For example, the lemma of \"running\" is \"run\" and \"better\" becomes \"good.\" Unlike stemming which simply removes prefixes or suffixes, it considers the word's meaning and part of speech (POS) and ensures that the base form is a valid word. This makes lemmatization more accurate as it avoids generating non-dictionary words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWrixOGY8qlF",
        "outputId": "fc7d1566-4a8e-41e9-c6a5-569fe6ff31a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentence:  The children are running towards a better place. They were including happily in the park and are now heading home. The sun was setting beautifully, casting long shadows. It was a perfect day for playing outside.\n",
            "Lemmatized Sentence:  The child are run towards a good place . They be include happily in the park and are now head home . The sun be set beautifully , cast long shadow . It be a perfect day for play outside .\n"
          ]
        }
      ],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"The children are running towards a better place. They were including happily in the park and are now heading home. The sun was setting beautifully, casting long shadows. It was a perfect day for playing outside.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "lemmatized_sentence = []\n",
        "\n",
        "for word, tag in tagged_tokens:\n",
        "    if word.lower() == 'are' or word.lower() in ['is', 'am']:\n",
        "        lemmatized_sentence.append(word)\n",
        "    else:\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, get_wordnet_pos(tag)))\n",
        "\n",
        "print(\"Original Sentence: \", sentence)\n",
        "print(\"Lemmatized Sentence: \", ' '.join(lemmatized_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrcZvkH-SHe"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Involves dividing a string or text into a list of smaller units known as tokens.\n",
        "Uses a tokenizer to segment unstructured data and natural language text into distinct chunks of information, treating them as different elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0l9mxYN9wbh",
        "outputId": "d2b8c192-e2f0-4d38-afc9-9c40c6863174"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to GeeksforGeeks.',\n",
              " 'You are studying NLP article.']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Sentence tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\"\n",
        "sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuPOeFVw-jNR",
        "outputId": "82201724-532e-4298-d858-871fbd79f891"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#word tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
        "word_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_h0Kqkn_IbV"
      },
      "source": [
        "### Stemming\n",
        "Stemming is an important text-processing technique that reduces words to their base or root form by removing prefixes and suffixes. This process standardizes words which helps to improve the efficiency and effectiveness of various natural language processing (NLP) tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAT9YYWG_XEc"
      },
      "source": [
        "### Porter's Stemmer\n",
        "Porter's Stemmer is one of the most popular and widely used stemming algorithms. Proposed in 1980 by Martin Porter, this stemmer works by applying a series of rules to remove common suffixes from English words. It is well-known for its simplicity, speed and reliability. However, the stemmed output is not guaranteed to be a meaningful word and its applications are limited to the English language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu99D-4P_DXt",
        "outputId": "140ee374-7356-413c-c4d5-9654961c4c77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original words: ['The', 'children', 'are', 'running', 'towards', 'a', 'better', 'place', '.', 'They', 'were', 'including', 'happily', 'in', 'the', 'park', 'and', 'are', 'now', 'heading', 'home', '.', 'The', 'sun', 'was', 'setting', 'beautifully', ',', 'casting', 'long', 'shadows', '.', 'It', 'was', 'a', 'perfect', 'day', 'for', 'playing', 'outside', '.']\n",
            "Stemmed words: ['the', 'children', 'are', 'run', 'toward', 'a', 'better', 'place', '.', 'they', 'were', 'includ', 'happili', 'in', 'the', 'park', 'and', 'are', 'now', 'head', 'home', '.', 'the', 'sun', 'wa', 'set', 'beauti', ',', 'cast', 'long', 'shadow', '.', 'it', 'wa', 'a', 'perfect', 'day', 'for', 'play', 'outsid', '.']\n"
          ]
        }
      ],
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"The children are running towards a better place. They were including happily in the park and are now heading home. The sun was setting beautifully, casting long shadows. It was a perfect day for playing outside.\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Apply stemming to each word\n",
        "stemmed_sentence = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNgQ9uw5A1DV"
      },
      "source": [
        "### POS Tagging\n",
        "Parts of Speech (PoS) tagging is a fundamental task in Natural Language Processing (NLP) where each word in a sentence is assigned a grammatical category such as noun, verb, adjective or adverb. This process help machines to understand the structure and meaning of sentences by identifying the roles of words and their relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr3pxByLA5fD",
        "outputId": "213cdcbf-b660-4951-cc4c-d5d62c26d7f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "NLTK is a powerful library for natural language processing.\n",
            "\n",
            "PoS Tagging Result:\n",
            "NLTK: NNP\n",
            "is: VBZ\n",
            "a: DT\n",
            "powerful: JJ\n",
            "library: NN\n",
            "for: IN\n",
            "natural: JJ\n",
            "language: NN\n",
            "processing: NN\n",
            ".: .\n"
          ]
        }
      ],
      "source": [
        "text = \"NLTK is a powerful library for natural language processing.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "\n",
        "print(\"\\nPoS Tagging Result:\")\n",
        "for word, pos_tag in pos_tags:\n",
        "    print(f\"{word}: {pos_tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3SyPLY-ARti"
      },
      "source": [
        "### Normalization\n",
        "transforms raw text into a consistent, standardized format for Natural Language Processing (NLP) models by handling variations like capitalization, punctuation, contractions, and word forms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9uVKNEjBu7u"
      },
      "source": [
        "### COMPLETE PIPELINE OF TEXT PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3bc76ef6"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    \"\"\"Removes HTML tags from text.\"\"\"\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    \"\"\"Removes accented characters from text.\"\"\"\n",
        "    normalized_text = unicodedata.normalize('NFD', text)\n",
        "    ascii_text = normalized_text.encode('ascii', 'ignore').decode('utf-8')\n",
        "    return ascii_text\n",
        "\n",
        "def expand_contractions(text):\n",
        "    \"\"\"Expands contractions in text.\"\"\"\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def text_lowercase(text):\n",
        "    \"\"\"Converts text to lowercase.\"\"\"\n",
        "    return text.lower()\n",
        "\n",
        "def remove_numbers(text):\n",
        "    \"\"\"Removes numbers from text.\"\"\"\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Removes punctuation from text.\"\"\"\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    \"\"\"Removes special characters, keeping only letters and spaces.\"\"\"\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "def remove_extra_whitespaces(text):\n",
        "    \"\"\"Removes extra whitespaces from text.\"\"\"\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "def remove_stopwords_func(text):\n",
        "    \"\"\"Removes English stopwords from text.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"Maps POS tags to WordNet tags for lemmatization.\"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \"\"\"Lemmatizes text.\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    # Explicitly reference pos_tag from nltk.tag\n",
        "    tagged_tokens = nltk.tag.pos_tag(tokens)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_tokens]\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "def stem_text(text):\n",
        "    \"\"\"Stems text using Porter Stemmer.\"\"\"\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [porter_stemmer.stem(word) for word in tokens]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "\n",
        "def preprocess_text_pipeline(text, remove_stopwords=True, apply_lemmatization=False, apply_stemming=False):\n",
        "    \"\"\"\n",
        "    Applies a sequence of text preprocessing steps.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input string.\n",
        "        remove_stopwords (bool): Whether to remove stopwords.\n",
        "        apply_lemmatization (bool): Whether to apply lemmatization.\n",
        "        apply_stemming (bool): Whether to apply stemming.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed string.\n",
        "    \"\"\"\n",
        "    # Initial cleaning\n",
        "    cleaned_text = remove_html_tags(text)\n",
        "    cleaned_text = remove_accented_chars(cleaned_text)\n",
        "    cleaned_text = expand_contractions(cleaned_text)\n",
        "    cleaned_text = text_lowercase(cleaned_text)\n",
        "    cleaned_text = remove_numbers(cleaned_text)\n",
        "    cleaned_text = remove_punctuation(cleaned_text)\n",
        "    cleaned_text = remove_special_characters(cleaned_text)\n",
        "    cleaned_text = remove_extra_whitespaces(cleaned_text)\n",
        "\n",
        "    # Optional steps\n",
        "    if remove_stopwords:\n",
        "        cleaned_text = remove_stopwords_func(cleaned_text)\n",
        "\n",
        "    # Note: Usually you would apply either lemmatization or stemming, not both.\n",
        "    # Lemmatization is generally preferred for better results but is computationally more expensive.\n",
        "    if apply_lemmatization and not apply_stemming:\n",
        "        cleaned_text = lemmatize_text(cleaned_text)\n",
        "    elif apply_stemming and not apply_lemmatization:\n",
        "        cleaned_text = stem_text(cleaned_text)\n",
        "    elif apply_lemmatization and apply_stemming:\n",
        "         print(\"Warning: Both lemmatization and stemming are set to True. Applying only lemmatization.\")\n",
        "         cleaned_text = lemmatize_text(cleaned_text)\n",
        "\n",
        "\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "446c1be5",
        "outputId": "4b8ffc04-04e0-44fe-d9ab-37799e5b41c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text (first 500 characters):\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Text Scraping with Python: A Step-by-Step Guide\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    Accessibility Menu  \n",
            "\n",
            "    skip to content  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign-up now\n",
            "            and we’ll match your first deposit dollar for dollar,\n",
            "            up to $500! \n",
            "\n",
            "\n",
            "\n",
            "              Start Now\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "        en        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Français\n",
            "日本語\n",
            "Português\n",
            "Русский\n",
            "简体中文\n",
            "English\n",
            "Español\n",
            "Deutsch\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "        Start Free Trial      \n",
            "\n",
            "\n",
            "\n",
            "        User Dashboard      \n",
            "\n",
            "\n",
            "\n",
            "Pr...\n",
            "\n",
            "Processed Text (with stopword removal and lemmatization, first 500 characters):\n",
            "text scrap python stepbystep guide accessibility menu skip content signup well match first deposit dollar dollar start en francais portugues english espanol deutsch start free trial user dashboard product web access apis unlocker apisay goodbye block captchas crawl apiturn entire website aifriendly data serp apiget multiengine search result ondemand browser apispin remote browser stealth include data feed scrapersfetch realtime data website linkedin ecomm social medium custom datasetsprecollecte...\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL to scrape\n",
        "url = \"https://brightdata.com/blog/web-data/text-scraping\"\n",
        "\n",
        "try:\n",
        "    # Fetch the webpage content\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract text content (you might want to refine this based on specific elements)\n",
        "    scraped_text = soup.get_text()\n",
        "\n",
        "    # Apply the preprocessing pipeline\n",
        "    processed_text = preprocess_text_pipeline(scraped_text, remove_stopwords=True, apply_lemmatization=True)\n",
        "\n",
        "    print(\"Original Text (first 500 characters):\")\n",
        "    print(scraped_text[:500] + \"...\")\n",
        "    print(\"\\nProcessed Text (with stopword removal and lemmatization, first 500 characters):\")\n",
        "    print(processed_text[:500] + \"...\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching the URL: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during processing: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6hEe-1cDDxh",
        "outputId": "54b39b39-300f-4c49-d040-a30ae4a57b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "\n",
            "<p>This is an example with <b>HTML tags</b> and some numbers like 123.\n",
            "It also includes contractions like I'll and isn't, and accented characters such as café and résumé.\n",
            "There are extra spaces here and there. Let's see how the pipeline handles punctuation!\n",
            "Running and jumped are different forms of verbs. Better is an adjective.\n",
            "</p>\n",
            "\n",
            "\n",
            "Processed Text (with stopword removal and lemmatization):\n",
            "example html tag number like also include contraction like accented character cafe resume extra space let u see pipeline handle punctuation run jump different form verbs well adjective\n",
            "\n",
            "Processed Text (with stopword removal and stemming):\n",
            "exampl html tag number like also includ contract like accent charact cafe resum extra space let us see pipelin handl punctuat run jump differ form verb better adject\n",
            "\n",
            "Processed Text (without stopword removal and with lemmatization):\n",
            "this be an example with html tag and some number like it also include contraction like i will and be not and accented character such a cafe and resume there be extra space here and there let u see how the pipeline handle punctuation run and jumped be different form of verb better be an adjective\n"
          ]
        }
      ],
      "source": [
        "# Example Usage:\n",
        "sample_text = \"\"\"\n",
        "<p>This is an example with <b>HTML tags</b> and some numbers like 123.\n",
        "It also includes contractions like I'll and isn't, and accented characters such as café and résumé.\n",
        "There are extra spaces here and there. Let's see how the pipeline handles punctuation!\n",
        "Running and jumped are different forms of verbs. Better is an adjective.\n",
        "</p>\n",
        "\"\"\"\n",
        "\n",
        "processed_text = preprocess_text_pipeline(sample_text, remove_stopwords=True, apply_lemmatization=True)\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nProcessed Text (with stopword removal and lemmatization):\")\n",
        "print(processed_text)\n",
        "\n",
        "processed_text_stemming = preprocess_text_pipeline(sample_text, remove_stopwords=True, apply_stemming=True)\n",
        "print(\"\\nProcessed Text (with stopword removal and stemming):\")\n",
        "print(processed_text_stemming)\n",
        "\n",
        "processed_text_no_stopwords = preprocess_text_pipeline(sample_text, remove_stopwords=False, apply_lemmatization=True)\n",
        "print(\"\\nProcessed Text (without stopword removal and with lemmatization):\")\n",
        "print(processed_text_no_stopwords)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
