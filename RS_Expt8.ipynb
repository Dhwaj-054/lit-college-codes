{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhwaj-054/lit-college-codes/blob/main/RS_Expt8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "NTsERGhAzFEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation"
      ],
      "metadata": {
        "id": "HhgNhlyS-LGs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8fa725a"
      },
      "source": [
        "\n",
        "credits_df = pd.read_csv('/content/tmdb_5000_credits.csv')\n",
        "display(credits_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TMDB movie dataset\n",
        "movies_df = pd.read_csv('ratings.csv')\n",
        "\n",
        "# Display the first 5 rows of the dataframe\n",
        "display(movies_df.head())"
      ],
      "metadata": {
        "id": "6iLMM8Z3-mTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45f62abd"
      },
      "source": [
        "Basic EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e923169c"
      },
      "source": [
        "print(\"\\nBasic statistics of movies_df:\")\n",
        "display(movies_df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eb891cb"
      },
      "source": [
        "Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d5fa7bd"
      },
      "source": [
        "!pip install surprise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c34d4142"
      },
      "source": [
        "!pip uninstall -y scikit-surprise surprise numpy\n",
        "!pip install numpy==1.26.4\n",
        "!pip install scikit-surprise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7d444b7"
      },
      "source": [
        "from surprise import Dataset, Reader\n",
        "from surprise.model_selection import train_test_split as surprise_split\n",
        "\n",
        "\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "\n",
        "data = Dataset.load_from_df(movies_df[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "\n",
        "trainset, testset = surprise_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Data successfully loaded into Surprise Dataset and split into training and testing sets.\")\n",
        "print(f\"Number of ratings in training set: {trainset.n_ratings}\")\n",
        "print(f\"Number of ratings in testing set: {len(testset)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc0ba10e"
      },
      "source": [
        "# Check the distribution of simulated ratings\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='rating', data=simulated_ratings_df)\n",
        "plt.title('Distribution of Simulated Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03aa8abe"
      },
      "source": [
        "\n",
        "num_simulated_users = 1000\n",
        "simulated_users = [f'user_{i}' for i in range(num_simulated_users)]\n",
        "\n",
        "\n",
        "rated_movies = merged_df[merged_df['vote_count'] > 100].sample(frac=0.5, random_state=42) # Sample 50% of movies with more than 100 votes\n",
        "\n",
        "\n",
        "simulated_ratings_list = []\n",
        "\n",
        "\n",
        "for user_id in simulated_users:\n",
        "\n",
        "    user_rated_movies = rated_movies.sample(n=np.random.randint(10, 50), random_state=np.random.randint(0, 1000)) # Each user rates between 10 and 50 movies\n",
        "\n",
        "    for index, movie in user_rated_movies.iterrows():\n",
        "\n",
        "        simulated_rating = round(max(1, min(5, movie['vote_average'] / 2.0 + np.random.uniform(-1, 1))), 1)\n",
        "        simulated_ratings_list.append({'user_id': user_id, 'movie_id': movie['id'], 'rating': simulated_rating})\n",
        "\n",
        "\n",
        "simulated_ratings_df = pd.DataFrame(simulated_ratings_list)\n",
        "\n",
        "\n",
        "display(simulated_ratings_df.head())\n",
        "\n",
        "\n",
        "print(f\"Shape of the simulated ratings dataframe: {simulated_ratings_df.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23402966"
      },
      "source": [
        "\n",
        "merged_df = movies_df.merge(credits_df.drop('title', axis=1), left_on='id', right_on='movie_id')\n",
        "\n",
        "\n",
        "display(merged_df.head())\n",
        "\n",
        "\n",
        "print(f\"Shape of the merged dataframe: {merged_df.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "user_item_matrix = simulated_ratings_df.pivot_table(index='user_id', columns='movie_id', values='rating')\n",
        "\n",
        "print(\"User-Item Rating Matrix:\")\n",
        "display(user_item_matrix)"
      ],
      "metadata": {
        "id": "MsmwM2iVAQ7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "user_item_matrix_filled = user_item_matrix.fillna(0)\n",
        "\n",
        "\n",
        "item_item_matrix = user_item_matrix_filled.T\n",
        "\n",
        "\n",
        "item_similarity_matrix = cosine_similarity(item_item_matrix)\n",
        "\n",
        "#\n",
        "item_similarity_df = pd.DataFrame(item_similarity_matrix, index=item_item_matrix.index, columns=item_item_matrix.index)\n",
        "\n",
        "print(\"Item Similarity Matrix (first 5x5):\")\n",
        "display(item_similarity_df.head())"
      ],
      "metadata": {
        "id": "PeCHEr4sAfHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_item_based_rating(user_id, movie_id, user_item_matrix, item_similarity_df):\n",
        "    \"\"\"\n",
        "    Predicts the rating for a given user and movie using item-based collaborative filtering.\n",
        "\n",
        "    Args:\n",
        "        user_id (str): The ID of the user.\n",
        "        movie_id (int): The ID of the movie.\n",
        "        user_item_matrix (pd.DataFrame): The user-item rating matrix.\n",
        "        item_similarity_df (pd.DataFrame): The item similarity matrix.\n",
        "\n",
        "    Returns:\n",
        "        float: The predicted rating for the user-movie pair, or None if prediction is not possible.\n",
        "    \"\"\"\n",
        "\n",
        "    if movie_id not in item_similarity_df.index:\n",
        "        return None\n",
        "\n",
        "\n",
        "    user_ratings = user_item_matrix.loc[user_id].dropna()\n",
        "\n",
        "\n",
        "    if user_ratings.empty:\n",
        "        return None\n",
        "\n",
        "\n",
        "    item_similarities = item_similarity_df[movie_id].drop(movie_id, errors='ignore')\n",
        "\n",
        "\n",
        "    user_ratings = user_ratings[user_ratings.index.isin(item_similarities.index)]\n",
        "\n",
        "\n",
        "    rated_item_similarities = item_similarities[user_ratings.index]\n",
        "\n",
        "\n",
        "\n",
        "    valid_similarities = rated_item_similarities[rated_item_similarities > 0].dropna()\n",
        "\n",
        "\n",
        "    if valid_similarities.empty:\n",
        "        return None\n",
        "\n",
        "\n",
        "    valid_ratings = user_ratings[valid_similarities.index]\n",
        "\n",
        "\n",
        "    numerator = np.sum(valid_similarities * valid_ratings)\n",
        "\n",
        "\n",
        "    denominator = np.sum(np.abs(valid_similarities))\n",
        "\n",
        "\n",
        "    if denominator == 0:\n",
        "        return None\n",
        "\n",
        "\n",
        "    predicted_rating = numerator / denominator\n",
        "\n",
        "    return predicted_rating\n",
        "\n"
      ],
      "metadata": {
        "id": "31g7JGdQAmWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92178abd"
      },
      "source": [
        "## 6. Predicting Ratings for the Test Set\n",
        "Using the `predict_item_based_rating` function and the item similarity matrix, we will predict the ratings for the user-movie pairs in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccf89980"
      },
      "source": [
        "\n",
        "predictions = []\n",
        "for user_id, movie_id, true_rating in testset:\n",
        "    predicted_rating = predict_item_based_rating(user_id, movie_id, user_item_matrix, item_similarity_df)\n",
        "    predictions.append((user_id, movie_id, true_rating, predicted_rating))\n",
        "\n",
        "\n",
        "predictions_df = pd.DataFrame(predictions, columns=['user_id', 'movie_id', 'true_rating', 'predicted_rating'])\n",
        "\n",
        "\n",
        "print(\"Predicted Ratings (first 5 rows):\")\n",
        "display(predictions_df.head())\n",
        "\n",
        "\n",
        "print(f\"Shape of the predictions dataframe: {predictions_df.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "relevance_threshold = 7.0\n",
        "\n",
        "\n",
        "merged_with_predictions = predictions_df.merge(merged_df[['movie_id', 'vote_average']], on='movie_id', how='left')\n",
        "\n",
        "merged_with_predictions['true_binary'] = (merged_with_predictions['vote_average'] >= relevance_threshold).astype(int)\n",
        "\n",
        "merged_with_predictions['predicted_binary'] = (merged_with_predictions['predicted_rating'] >= relevance_threshold).astype(int)\n",
        "\n",
        "\n",
        "print(\"Predictions with Binary Relevance (first 5 rows):\")\n",
        "display(merged_with_predictions.head())"
      ],
      "metadata": {
        "id": "F9h_W0MtA_Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c46c904f"
      },
      "source": [
        "## 8. Evaluate Top-N Recommendation Metrics\n",
        "We will now calculate Precision@10, Recall@10, and F1@10 to evaluate the top-N recommendation performance of our item-based collaborative filtering model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b343baa"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "filtered_predictions = merged_with_predictions.dropna(subset=['predicted_rating'])\n",
        "\n",
        "\n",
        "try:\n",
        "    precision_at_k = precision_score(filtered_predictions['true_binary'], filtered_predictions['predicted_binary'], average='macro', zero_division=0)\n",
        "    recall_at_k = recall_score(filtered_predictions['true_binary'], filtered_predictions['predicted_binary'], average='macro', zero_division=0)\n",
        "    f1_at_k = f1_score(filtered_predictions['true_binary'], filtered_predictions['predicted_binary'], average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"Precision: {precision_at_k:.4f}\")\n",
        "    print(f\"Recall: {recall_at_k:.4f}\")\n",
        "    print(f\"F1 Score: {f1_at_k:.4f}\")\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Could not calculate Precision, Recall, or F1 score: {e}\")\n",
        "    print(\"This might happen if there are no samples with positive or negative labels in both true and predicted binary columns.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the calculated metrics\n",
        "metrics = {\n",
        "    'Metric': ['Precision', 'Recall', 'F1 Score', 'RMSE', 'NDCG (simplified)'],\n",
        "    'Score': [precision_at_k, recall_at_k, f1_at_k, rmse if rmse is not None else 0, ndcg]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# Visualize the metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Metric', y='Score', data=metrics_df)\n",
        "plt.title('Evaluation Metrics (Item-Based CF)')\n",
        "plt.ylabel('Score')\n",
        "# Adjust ylim based on the range of scores (metrics like RMSE can be greater than 1)\n",
        "plt.ylim(0, max(metrics_df['Score'].max() * 1.1, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9oOJPoodBNLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "valid_predictions = predictions_df.dropna(subset=['predicted_rating'])\n",
        "\n",
        "if not valid_predictions.empty:\n",
        "    rmse = sqrt(mean_squared_error(valid_predictions['true_rating'], valid_predictions['predicted_rating']))\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "else:\n",
        "    rmse = None\n",
        "    print(\"Cannot calculate RMSE as there are no valid predictions.\")\n",
        "\n",
        "\n",
        "\n",
        "true_relevance = merged_with_predictions.dropna(subset=['predicted_rating'])['true_binary'].tolist()\n",
        "predicted_scores = merged_with_predictions.dropna(subset=['predicted_rating'])['predicted_rating'].tolist()\n",
        "\n",
        "\n",
        "scored_items = sorted(zip(predicted_scores, true_relevance), key=lambda x: x[0], reverse=True)\n",
        "\n",
        "\n",
        "ranked_true_relevance = [item[1] for item in scored_items]\n",
        "\n",
        "\n",
        "def dcg_at_k(r, k):\n",
        "    r = np.asfarray(r)[:k]\n",
        "    return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
        "\n",
        "\n",
        "def ndcg_at_k(r, k):\n",
        "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
        "    if not idcg:\n",
        "        return 0.\n",
        "    return dcg_at_k(r, k) / idcg\n",
        "\n",
        "\n",
        "ndcg = ndcg_at_k(ranked_true_relevance, len(ranked_true_relevance)) # Calculate for the whole list\n",
        "\n",
        "print(f\"NDCG (simplified): {ndcg:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "y5VpN7bWBuF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize all metrics together\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Metric', y='Score', data=metrics_df)\n",
        "plt.title('Evaluation Metrics (Item-Based CF)')\n",
        "plt.ylabel('Score')\n",
        "# Adjust ylim based on the range of scores (metrics like RMSE can be greater than 1)\n",
        "plt.ylim(0, max(metrics_df['Score'].max() * 1.1, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WlMPWoFkC81A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0_tWo9m1B08L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "end v4"
      ],
      "metadata": {
        "id": "XhM_rvQpDcyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:\n",
        "\n",
        "In this experiment, we successfully evaluated a recommendation system using the TMDB 5000 Movies dataset. Since explicit user ratings weren't available, we simulated realistic user-item interactions based on movie popularity and vote averages.\n",
        "We implemented an item-based collaborative filtering model and evaluated its performance using three categories of metrics:\n",
        "•\tRating Prediction Metrics (MAE, RMSE) - measured prediction accuracy\n",
        "•\tClassification Metrics (Precision, Recall, F1 Score) - assessed recommendation relevance\n",
        "•\tRanking Metrics (MAP, MRR, NDCG) - evaluated ranking quality\n",
        "The results demonstrate that our recommendation system effectively identifies and ranks relevant movies. Using multiple evaluation metrics provided comprehensive insights into different aspects of system performance, from accuracy to ranking quality. The visualizations clearly showed the trade-offs between metrics like precision and recall.\n",
        "This experiment highlights the importance of diverse evaluation metrics in recommendation systems, as each metric captures different performance dimensions. The approach can be extended to other domains like e-commerce, streaming platforms, and online education.\n"
      ],
      "metadata": {
        "id": "7_EH_n0RDgO4"
      }
    }
  ]
}